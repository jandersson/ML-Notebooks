{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "A binary classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Prediction\n",
    "\n",
    "To perform a prediction using logistic regression we use the following formula: \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{y} = \\sigma (w^Tx + b)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\end{equation}\n",
    "\n",
    "$\\sigma(z)$: Also called the sigmoid function. The output will never be greater than one\n",
    "\n",
    "$\\hat{y}^{(i)}$: The predicted label for example $i$\n",
    "\n",
    "$w$: Vector of weights\n",
    "\n",
    "$b$: bias (a real number)\n",
    "\n",
    "$x^{(i)}$: The $i$th input or training example\n",
    "\n",
    "${y}^{(i)}$: Ground truth label for example $i$\n",
    "\n",
    "We want $\\hat{y}$ to be as close as possible to $y$, or $\\hat{y} \\approx y$\n",
    "\n",
    "## Loss (error) function $\\mathcal{L}$\n",
    "\n",
    "\\begin{equation*}\n",
    "\\mathcal{L}(\\hat{y}^{(i)},y^{(i)}) = -(y^{(i)}\\log{\\hat{y}^{(i)}} + (1 - y^{(i)}) \\log({1 - \\hat{y}^{(i)}}))\n",
    "\\end{equation*}\n",
    "\n",
    "This function will measure how close our output $\\hat{y}$ is to the true label $y$.\n",
    "\n",
    "### Intuition \n",
    "We want $\\mathcal{L}$ to be as small as possible. \n",
    "\n",
    "If $y = 1$ then $\\mathcal{L}(\\hat{y},y) = - \\log \\hat{y}$. \n",
    "\n",
    "The second expression cancels out because $(1-1) = 0$. So now we want $- \\log \\hat{y}$ to be as large as possible. That means we want $\\hat{y}$ to be large. The sigmoid function above, $\\sigma (z)$ can never be greater than one.\n",
    "\n",
    "If $y = 0$ then $\\mathcal{L}(\\hat{y},y) = - \\log (1 - \\hat{y})$\n",
    "\n",
    "Similar reasoning, now we want $\\hat{y}$ as small as possible because we still want $\\log 1 -\\hat{y}$ large\n",
    "\n",
    "Another option is to use the squared error function: $\\mathcal{L}(\\hat{y},y) = \\frac{1}{2}(\\hat{y} - y)^2$  But this will produce a non-convex surface which is not good for gradient descent because it may not find the global optimum, but rather only a local optimum.\n",
    "\n",
    "## Cost function $J$ used in Logistic Regression\n",
    "\n",
    "\\begin{equation*}\n",
    "J(w,b) = - \\frac{1}{m}\\sum_{i=1}^m \\mathcal{L}(\\hat{y}^{(i)},y^{(i)})\n",
    "\\end{equation*}\n",
    "\n",
    "The cost function is the average, the sum, over the loss functions, divided by the number of examples $m$. This measures how well your parameters, $w$ and $b$ are performing on the training set. So the optimization problem is to minimize $J(w,b)$. We want to find $w$ and $b$ that make $J$ as small as possible.\n",
    "\n",
    "## Gradient Descent\n",
    "\n",
    "_Repeat until convergence:_\n",
    "\\begin{equation}\n",
    "w = w - \\alpha \\frac{\\partial J(w, b)}{\\partial w}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "b = b - \\alpha \\frac{\\partial J(w, b)}{\\partial b}\n",
    "\\end{equation}\n",
    "\n",
    "$\\alpha$: Learning rate. How big of a step we take towards the minimum with each iteration\n",
    "\n",
    "### Initializing $w$\n",
    "Typically this is initialized to zero. Since the error surface is convex, it should arrive at the same minimum given any initialization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
